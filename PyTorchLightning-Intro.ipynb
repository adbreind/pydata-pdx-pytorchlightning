{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcdee22-a64a-41df-beb0-8fb346dc8b93",
   "metadata": {},
   "source": [
    "# Enough with the `loss.backward()` already:<br/>DRY out your PyTorch code and gain superpowers with PyTorch Lightning\n",
    "\n",
    "PyTorch is famous for its \"what you see is what you get\" coding style\n",
    "* Define-by-run + implicit gradient calculation\n",
    "__BUT__\n",
    "* Explicit loss and backprop calculation\n",
    "* Explicit weight updates\n",
    "* Explicit training flow control\n",
    "* Explicit device targeting\n",
    "\n",
    "Let's review a quick example (data from https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8351b843-b1b5-4245-a6a0-9c5b659864b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('powerplant.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736bd502-530b-4c11-ab57-753f0aa256d9",
   "metadata": {},
   "source": [
    "We want to model PE (power output) as a function of the other variables. We'll use a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d69461-009a-4bdc-b54a-f459b464e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_pyt = torch.tensor(df[['AT', 'V', 'RH', 'AP']].values, dtype=torch.float)\n",
    "y_train_pyt = torch.tensor(df.PE.values, dtype=torch.float)\n",
    "\n",
    "size = len(df)\n",
    "val_size = size // 10\n",
    "\n",
    "X_train_pyt_main = X_train_pyt[:(size-val_size)]\n",
    "X_train_pyt_val = X_train_pyt[(size-val_size):]\n",
    "\n",
    "y_train_pyt_main = y_train_pyt[:(size-val_size)]\n",
    "y_train_pyt_val = y_train_pyt[(size-val_size):]\n",
    "\n",
    "train_ds = TensorDataset(X_train_pyt_main, y_train_pyt_main[:, None])\n",
    "print(len(train_ds))\n",
    "\n",
    "val_ds = TensorDataset(X_train_pyt_val, y_train_pyt_val[:,None])\n",
    "print(len(val_ds))\n",
    "\n",
    "def train(training, validation, epochs, batch_size, model, loss_fn, optimizer):\n",
    "    history = { 'train' : [], 'val' : [] }\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        for i in range((len(training) - 1) // batch_size + 1):\n",
    "            xb, yb = training[i * batch_size: i * batch_size + batch_size]      \n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        epoch_loss = pd.Series(batch_losses).mean()\n",
    "        history['train'].append(epoch_loss)\n",
    "        y_val_pred = model(validation.tensors[0])\n",
    "        epoch_val = loss_fn(y_val_pred, validation.tensors[1]).item()\n",
    "        history['val'].append(epoch_val) \n",
    "        print(\"MSE for epoch {} = Train {}, Val {}\".format(epoch, epoch_loss, epoch_val))     \n",
    "    return history\n",
    "\n",
    "D_in, H1, D_out = 4, 8, 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  torch.nn.Linear(D_in, H1), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(H1, D_out)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46c3b9-558c-4070-a529-540a17b540a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train(train_ds, val_ds, 10, 50, model, torch.nn.MSELoss(), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d92ee7-0009-4b76-b40e-e4fcdc456c7d",
   "metadata": {},
   "source": [
    "__This is a lot of boilerplate ... and this example (since it's meant to run in binder) doesn't even get into device targeting__\n",
    "\n",
    "For many common use cases, \n",
    "\n",
    "1. We don't actually need (or want) that level of control/visibility\n",
    "2. More code == more long-term maintainability cost\n",
    "\n",
    "## Goal: DRY and Automate: Reducing/Removing Boilerplate, add Functionality\n",
    "\n",
    "Now we want to simplify/automate that as well as provide more functionality like:\n",
    "* Drive visual reporting tools like Tensorboard\n",
    "* Integrate hooks for checkpointing, early stopping, etc.\n",
    "* Prepare for distributed training and/or deploying to alternative hardware\n",
    "\n",
    "In the past, there have been several tools that help with this...\n",
    "* skorch https://github.com/skorch-dev/skorch\n",
    "* Torchbearer https://github.com/pytorchbearer/torchbearer\n",
    "* Ignite https://pytorch.org/ignite/index.html\n",
    "* and a bunch of smaller projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b15d3-9a26-4f2d-8629-e33eb16bb82d",
   "metadata": {},
   "source": [
    "## Introducing PyTorch Lightning\n",
    "\n",
    "Today, __PyTorch Lightning__ (https://www.pytorchlightning.ai/) has emerged as the dominant best-practices extension of PyTorch to meet our training and refactor needs.\n",
    "\n",
    "> The ultimate PyTorch research framework. Scale your models, without the boilerplate.\n",
    "\n",
    "Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6a136-08f0-425f-844d-c447550bed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf9cf0-1250-4b90-b1f2-7a6dd1dec63b",
   "metadata": {},
   "source": [
    "## Where do we start with PyTorch Lightning?\n",
    "\n",
    "Since we already have it installed and verified that it can load, the next step is\n",
    "\n",
    "### Create a Lightning Module\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. Define a class that inherits from `pl.LightningModule` (a Lightning Module is actually just a flavor or regular PyTorch module!)\n",
    "2. In the constructor, put our neural-net-building code (assign the model to an instance variable)\n",
    "3. Add a `forward` method for the forward pass (return the output of model)\n",
    "4. Add a `training_step` method (return the loss)\n",
    "5. Add a `configure_optimizers` method (return your optimizer instance)\n",
    "    \n",
    "__We'll do this part one step at a time__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758e9e3-6d62-4d1f-b7f7-41d120c71393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPowerplant(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        D_in, H1, D_out = 4, 8, 1\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H1), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H1, D_out)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41eddc-39f9-4c32-89b6-8b02b18d641d",
   "metadata": {},
   "source": [
    "Add `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1251173-aa44-4fc3-85cd-0d90578244d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPowerplant(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        D_in, H1, D_out = 4, 8, 1\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H1), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H1, D_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d008f-e0e3-4cf2-9bed-dfb08027a3eb",
   "metadata": {},
   "source": [
    "Next, `training_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a9a74-591a-4464-b410-0a94e4496da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPowerplant(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        D_in, H1, D_out = 4, 8, 1\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H1), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H1, D_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss = loss_fn(pred, y)\n",
    "        self.log('train_loss', loss) #TensorBoard by default\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592977a0-f1ca-4c82-91cc-1619d3a1dd92",
   "metadata": {},
   "source": [
    "And then `configure_optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf89d2-8897-4c02-8877-73de23d5c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPowerplant(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        D_in, H1, D_out = 4, 8, 1\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H1), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H1, D_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss = loss_fn(pred, y)\n",
    "        self.log('train_loss', loss) #TensorBoard by default\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        return optimizer\n",
    "    \n",
    "    # We can also add our validation and/or test logic (later we'd DRY this code by factoring out the common codes from training)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss = loss_fn(pred, y)\n",
    "        self.log('val_loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca57b7a-e9b5-4bb5-9934-e5ffb282f3a7",
   "metadata": {},
   "source": [
    "### Prepare our Data via DataLoader\n",
    "\n",
    "This is a PyTorch pattern/best practice that we would often want or need to do anyway!\n",
    "\n",
    "DataLoader represents a Python iterable over a dataset, with support for\n",
    "* map-style and iterable-style datasets,\n",
    "* customizing data loading order,\n",
    "* automatic batching,\n",
    "* single- and multi-process data loading,\n",
    "* automatic memory pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a810f-6a08-4f96-ba01-83fec3a4bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "validate_dataloader = DataLoader(val_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd782efd-e70a-4d8a-a749-032d25f29118",
   "metadata": {},
   "source": [
    "### The Big Payoff: Lighting Training\n",
    "\n",
    "We are now ready to train with the Lightning `Trainer` object.\n",
    "\n",
    "Let's get it running and then review the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23abfc4-87ce-492c-9ce3-76261aadbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "powerplant = LitPowerplant()\n",
    "\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "trainer = pl.Trainer(max_epochs=12)\n",
    "trainer.fit(powerplant, train_dataloader, validate_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86678b25-2648-4982-8533-c3766c756673",
   "metadata": {},
   "source": [
    "### Ok, what have we got?\n",
    "\n",
    "__Under the hood, the Lightning Trainer handles the training loop details for you, some examples include:__\n",
    "* Epoch and batch iteration (running the training, validation and test dataloaders)\n",
    "* Calling of optimizer.step(), backward, zero_grad()\n",
    "* Calling of .eval(), enabling/disabling grads\n",
    "* Tensorboard (see loggers options)\n",
    "* GPU/Multi-GPU support, TPU, AMP support\n",
    "  * Putting batches and computations on the correct devices\n",
    "* Calling the Callbacks at the appropriate times (e.g., https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#checkpoint-callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b507d-b57a-4f2a-801a-6b7334acc66c",
   "metadata": {},
   "source": [
    "### Viewing Tensorboard\n",
    "\n",
    "If you're running locally (and you have TF installed), you can launch\n",
    "\n",
    "`tensorboard --logdir ./lightning_logs`\n",
    "\n",
    "And in Google Colab, we can use a notebook extension to render Tensorboard inline (for more info see https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68db7c-cb73-4c5b-b4b6-f04193a3d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570c8e6-45af-4984-a859-92db4fd405a2",
   "metadata": {},
   "source": [
    "### One Last Bonus\n",
    "\n",
    "Simple distributed training with Ray (https://www.ray.io/) and Ray Lightning (https://github.com/ray-project/ray_lightning)\n",
    "\n",
    "```python\n",
    "! pip install ray_lightning\n",
    "from ray_lightning import RayPlugin\n",
    "\n",
    "plugin = RayPlugin(num_workers=100, use_gpu=False, num_cpus_per_worker=1)\n",
    "\n",
    "# Don't set `gpus` -- the actual number of GPUs is determined by `num_workers`\n",
    "\n",
    "ray_trainer = pl.Trainer(max_epochs=100, plugins=[plugin])\n",
    "ray_trainer.fit(powerplant, train_dataloader, validate_dataloader)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd41a08-b5ae-4c01-a040-c54cedbbe1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
